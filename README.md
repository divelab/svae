# Spatial VAE via Matrix-Variate Normal Distributions

This is the tensorflow implementation of our recent work, "Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions". Please check the [paper](https://arxiv.org/abs/1705.06821) for details.

In this work, we propose spatial VAEs that use latent variables as feature maps of larger size to explicitly capture spatial information. This is achieved by allowing the latent variables to be sampled from matrix-variate normal (MVN) distributions whose parameters are computed from the encoder network.

## Experimental results:
1. CelebA dataset

![image](https://github.com/divelab/Spatial-VAE-via-MVND/blob/master/results/celeba_new.png)

2. Cifar dataset

![image](https://github.com/divelab/Spatial-VAE-via-MVND/blob/master/results/cifar_new.png)


In both figures above, the first and second rows shows training images and images generated by the original VAEs. The
remaining three rows are the results of naïve spatial VAEs, spatial VAEs via MVN distributions and
spatial VAEs via low-rank MVN distributions, respectively.

For celebA dataset, it is clear that spatial VAEs can generate images with more details than the original
VAEs. Due to the lack of explicit spatial information, the original VAEs produce face images with little
details like hair near the borders. While naïve spatial VAEs seem to address this problem, most faces
have only incomplete hairs as naïve spatial VAEs cannot capture the relationships among different
locations. Theoretically, spatial VAEs via MVN distributions are able to incorporate interactions
among locations. However, the results are strange faces with some distortions. We believe the
reason is that adding dependencies among locations through restrictions on distribution variances is
not effective and sufficient. To tackle this, spatial VAEs via low-rank MVN distributions that have
restricted means are proposed and generate faces with appealing visual appearances.

For cifar dataset, the original VAEs only produce images composed of several colored areas. It is
obvious that all three implementations of spatial VAEs generate images with more details.


## Datasets:

Our experiments are based on [cifar 10](https://www.cs.toronto.edu/~kriz/cifar.html) and [cropped celebA datasets](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). We provide data reader for those two datasets. For celebA dataset, please convert it to h5 file first then call the data reader.

You can use other datasets such as [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/) or [LSUN Dataset](http://lsun.cs.princeton.edu/2016/) as well; just write a simple data reader file. 

## How to run it

1. Clone or download this repository to your working directory;
2. Get the datasets ready;
3. Set arguments in main.py, set "model_name" as 'vanilla' to call tranditional VAE and use 'low_rank' to call our low_rank VAE;
4. Call ``` python main.py ``` or  ``` python main.py --action=train ``` to train the model;
5. If you wish to use "parzen window" to evaluate the model, set a checkpoint in arguments for the model to reload and then call ``` python main.py --action=test```.






